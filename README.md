# 中文分词算法总结

Xu Jing

自中文自动分词被提出以来，历经将近30年的探索，提出了很多方法，可以归结为：“规则分词”，“统计分词”，“混合分词（规则+统计）这三个主要流派。规则分词是最早兴起的方法，主要通过设立人工词库，按照一定方式进行切分，简单高效，但是对新词很难进行处理。随后统计机器学习的技术兴起，应用于分词任务之后，就有了统计分词，可以较好的应对新词发现等特殊场景。实践中，单纯的统计分词也有缺陷，那就是太过于依赖语料的质量，实践中多是采用两种方法的结合，即混合分词。

## 规则分词

**正向最大匹配法（Maximum Match Method,MM方法）**的基本思想为：假定分词词典中的最长的词有i个汉字字符，则用被处理文档的当前字串中的前i个字作为匹配字段，查找字典。若字典中存在这样一个i字词，则匹配成功，匹配字段被作为一个词切分出来。如果词典中找不到这样一个i字词，则匹配失败，将匹配字段中的最后一个字去掉，将剩下的字串重新进行匹配处理。如此进行下去，直到匹配成功，即切分出一个词或剩余字串的长度为0为止。这样就完成了一轮匹配，然后取下一个i字字串进行匹配处理，直到文档被扫描完为止。

**逆向最大匹配法（Reverse Maximum Match Method,RMM法）**的基本原理与MM相同，不同的是分词切分的方向与MM法相反。逆向最大匹配法从被处理的末端开始匹配扫描，每次取最末端的i个字符（i为词典中最长词数）作为匹配字段，若匹配失败，则去掉匹配字段最前面的一个字，继续匹配。相应的，他是用的分词词典是逆序词典，其中的每个词条都将按逆序方式存放。在实际处理时，先将文档进行倒排处理，生成逆序文档。然后，根据逆序词典，对逆序文档用正向最大匹配法处理即可。

因为汉语偏正结构较多，若从后向前匹配则可以适当的提高精确度，所以逆向最大匹配法比正向最大匹配法的误差要小。

**双向最大匹配法（Bi-directction Matchong method）**是将正向最大匹配法得到的分词结果和逆向最大匹配法得到的结果进行比较，然后按照最大匹配原则，选取词数切分最少的作为结果。例如：“南京市长江大桥”，采用该方法，中间产生：“南京市/长江/大桥”和“南京市/长江大桥”两种结果，最终选取词数较少的“南京市/长江大桥”作为结果。

## 统计分词


## 混合分词

事实上，目前不管是基于规则的算法，还是基于HMM,CRF，或者DeepLearning等的方法，其实分词效果在具体的任务中，差距并没有那么明显，在实际中多是基于一种分词算法，然后用其他分词算法加以辅助。

Python模块：

jieba、THULAC、SnowNLP、pynlpir、CoreNLP、pyLTP（哈工大）、NLPIR